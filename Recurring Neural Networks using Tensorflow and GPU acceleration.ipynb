{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52b24143",
   "metadata": {},
   "source": [
    "# **Deep Learning Fundamentals: RNN**\n",
    "\n",
    "# What are Recurrent Neural Networks?\n",
    "\n",
    "RNN is a variety of Artifical Neural Networks, and till date remain the only type of neural network with an internal memory.\n",
    "\n",
    "The genesis of RNN can be traced to as far back as the Ising model, devised in 1925, which dealt with ferromagnetism and was used to calculate spins in a lattice.\n",
    "\n",
    "Shunichi Amari's work on geometric structure in 1972 gave mathematicians the tools to understand neural networks.\n",
    "\n",
    "Hopfield network and David Rumelhart's work in 1980's helped further refine this model, and it finally culminated in the creation of LSTM networks in 1997.\n",
    "\n",
    "Since they possess an internal memory, RNN's remain the only neural network that can be used for natural language processing and for predicting events and trends like stock market, weather etc.\n",
    "\n",
    "# How do RNN's work?\n",
    "\n",
    "RNN's depend on sequential data to function. Sequential data is, in essence, ordered data in which related things follow each other. Some examples are financial data, Protein sequencing or DNA sequencing. \n",
    "\n",
    "![](./Images/2024-03-01-22-08-31.png)\n",
    "\n",
    "RNN's are a variant of ANN, which are a feed-forward neural network. \n",
    "\n",
    "A feed-forward Neural network channels information in only one direction: from the input layer, through the hidden layers to the output layer. The information moves straight through the network. \n",
    "\n",
    "Therefore, feed forward neural networks have no memory of the input they recieve and are thus incapable of making any predictions. Since feed forward networks only focus on the current input, it has no notion of order in respect to time.\n",
    "\n",
    "in RNN, however, this information travels in a loop. When a decision is made, the current input is considered and what was learned recently is also factored into the next decisions.\n",
    "\n",
    "![](./Images/2024-03-01-22-09-19.png)\n",
    "\n",
    "# Types of Recurrent neural networks\n",
    "\n",
    "the types of neural networks are:\n",
    "\n",
    "- One to One\n",
    "- One to Many\n",
    "- Many to One\n",
    "- Many to Many\n",
    "\n",
    "![](./Images/2024-03-03-06-38-23.png)\n",
    "\n",
    "# RNN and Backpropagation through time (BPTT)\n",
    "\n",
    "In Neural networks, each neuron can be thought of as a block of mathematical calculations with an input and an output. What makes each of these neurons different is the weights attached to them. This allows the neural network to explore different solutions to reduce error.\n",
    "\n",
    "Forward propagation refers to the neurons from the previous layer providing output, which is used by neurons in the next layer as input. Calculations are done and error is obtained.\n",
    "\n",
    "In backpropagation, based on the error calculated by the neural network, the model goes back through the neurons to find the partial derivatives of the error with respect to the weights given to these neurons. These weights are then used to decrease error margins.\n",
    "\n",
    "![](./Images/2024-03-01-22-11-06.png)\n",
    "\n",
    "For development and troubleshooting purposes, RNN's are sometimes \"unrolled\" to be able visualize what is going on in a model in order to make tweaks and adjustments.\n",
    "\n",
    "![](./Images/2024-03-01-22-12-01.png)\n",
    "\n",
    "# Pros and Cons of RNN\n",
    "\n",
    "**Pros**\n",
    "\n",
    "- It can process input of any length.\n",
    "- Model size doesn't increase with increase in input size.\n",
    "- Computation will take into account older information.\n",
    "- Weights are shared across time.\n",
    "\n",
    "**Cons**\n",
    "\n",
    "- Computation is slow, because computation is sequential and cannot be done in parallel.\n",
    "- Model has a short term memory and therefore has trouble accessing information from a long time ago.\n",
    "- Model cannot consider any future input for the current state.\n",
    "\n",
    "# Limitations of RNN\n",
    "\n",
    "- **Exploding gradients:** this happens when the model assigns a very high importance to the weights all of a sudden. This problem is solved by truncating or squashing the gradients\n",
    "- **Vanishing gradients:** This happens when the gradient values are so low the model makes little to no changes to weights and causes the learning time to (theoretically) tend to infinity. This can cause models to either stop learning or take too long to learn. LSTM was made to solve this issue.\n",
    "\n",
    "# Long Short Term Memory\n",
    "\n",
    "LSTM are an extension for RNN's which solve the vanishing gradients problem in the model. LSTM cells are used as basic building blocks in an RNN and have gates which decide which information to keep in and which information to discard by assigning them weights. This information flow is controlled by three gates:\n",
    "\n",
    "1. **Forget Gate**\n",
    "\n",
    "The Forget Gate plays a crucial role in deciding which information from the cell state at the current timestamp should be discarded. This decision is determined through the sigmoid function. Essentially, the Forget Gate helps the model to selectively \"forget\" certain pieces of information, making room for new inputs.\n",
    "\n",
    "2. **Input Gate**\n",
    "\n",
    "The Input Gate is responsible for determining how much of the new information should be added to the current state of the cell. The decision-making process involves two steps:\n",
    "\n",
    "- The sigmoid function decides which values to allow through (ranging from 0 to 1).\n",
    "- The tanh function assigns weights to the allowed values, indicating their level of importance. These weights range from -1 to 1.\n",
    "\n",
    "3. **Output Gate**\n",
    "\n",
    "The Output Gate is tasked with deciding which portion of the current cell state contributes to the output. Similar to the Input Gate, this decision is a two-step process:\n",
    "\n",
    "- The sigmoid function determines which values should be permitted through (ranging from 0 to 1).\n",
    "- The tanh function assigns weights to the allowed values, indicating their level of importance and ranging from -1 to 1. This output is further expanded with the output of a sigmoid function.\n",
    "\n",
    "In essence, the Output Gate regulates the information that is passed on to the model's output, and considers both the relevance and significance of the current cell state. \n",
    "\n",
    "![](./Images/2024-03-03-06-59-28.png)\n",
    "\n",
    "![](./Images/2024-03-01-22-12-39.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568002d0",
   "metadata": {},
   "source": [
    "# RNN Project: Using Sherlock Holmes to predict a sentence\n",
    "\n",
    "For this project, we will be demonstrating the Natural Language Processing capabilities of RNN by using a story from Arthur Conan Doyle's Sherlock Holmes Series: The Final Problem, which sees Sherlock meeting his match against professor Moriarty.\n",
    "\n",
    "**_Why use this data set?_**\n",
    "\n",
    "Correct utilization of AI depends as much on good data as it does on the appropriate model.\n",
    "\n",
    "A story, like The Final Problem, has all use-cases of natural language:\n",
    "\n",
    "- Dialogue\n",
    "- Internal monologue (thinking)\n",
    "- Exposition (description of things like events, actions, feelings)\n",
    "\n",
    "therefore, using stories as datasets can provide the RNN model with a wide variety of sentences in a wide variety of contexts within a self-contained unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e701c2a5",
   "metadata": {},
   "source": [
    "# Step 0: Enabling GPU support\n",
    "\n",
    "Since we will be using a high number of epochs (100) to train our model, and be using a high number of neurons (150), it will lead to a long training time on default hardware allocations. \n",
    "\n",
    "By default, Tensorflow uses the CPU to train the algorithm. By using CUDA, we can greatly enhance our training speed and be able to use bigger datasets and higher epoch numbers. \n",
    "\n",
    "to enable GPU support, we require CUDA Toolkit and cuDNN from Nvidia.\n",
    "\n",
    "However, Tensorflow 2.10 is the last version of tensorflow that supported native GPU usage. Later versions required the use of WSL (Windows Subsystem for Linux), a Linux emulator.\n",
    "\n",
    "Therefore to use GPU on Tensorflow natively, we will be creating a custom anaconda environment. \n",
    "\n",
    "To create a custom Anaconda environment, we open the Anaconda Command prompt with Administrator privileges to ensure sufficient write privileges.\n",
    "\n",
    "the commands to create a custom anaconda environment are:\n",
    "\n",
    " > conda create py310 python=3.10\n",
    "\n",
    "now we activate our python environment\n",
    "\n",
    " > conda activate py310\n",
    "\n",
    "now we are in our python environment and will be installing all the needed modules and libraries.\n",
    "\n",
    " > conda install -c  conda-forge cudatoolkit=11.2 cudnn=8.1.0\n",
    "\n",
    "And finally, we install the desired version of tensorflow.\n",
    " \n",
    " > python -m pip install \"tensorflow=2.10\"\n",
    "\n",
    " To test whether the GPU is being detected, we use the following program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "955a6279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU devices:\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check for the presence of GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if not gpus:\n",
    "    print(\"No GPU devices found.\")\n",
    "else:\n",
    "    print(\"Available GPU devices:\")\n",
    "    for gpu in gpus:\n",
    "        print(gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c579c5e",
   "metadata": {},
   "source": [
    "We have now confirmed the presence of our GPU and that it can be used by Tensorflow. We don't need to specify the program to use tensorflow as it can assign hardware automatically.\n",
    "\n",
    "We can confirm usage of GPU by running a sample model to train and viewing the resource usage in Task Manager:\n",
    "\n",
    "![](./Images/2024-03-03-06-04-32.png)\n",
    "\n",
    "The speed improvements are massive, reporting as high as 1/3rd the time per epoch. For example, here are epoch times on CPU, a Ryzen 7 7840HS:\n",
    "\n",
    "![](./Images/2024-03-03-06-05-42.png)\n",
    "\n",
    "and here are the epoch times for the same program, utilizing GPU acceleration on the RTX 4060 laptop GPU:\n",
    "\n",
    "![](./Images/2024-03-03-06-06-32.png)\n",
    "\n",
    "now we can continue with our project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab0c37e",
   "metadata": {},
   "source": [
    "# Step 1: Importing all the necessary libraries\n",
    "\n",
    "For the purpose of this project, we will be importing Tensorflow, an open source Machine Learning Library from Google. We will be using Keras, a well known module used for sequential data.\n",
    "\n",
    "From Tensorflow we will be importing the following modules\n",
    "\n",
    "- **Tokenizer:** to convert the data into tokens with unique indices\n",
    "- **pad_sequences:** to equalize the length of all sequences of tokens\n",
    "- **Sequential, Embedding, LSTM, Dense:** to build our model\n",
    "\n",
    "We will also be importing Numpy for dealing with data in the form of lists, arrays etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dec2769e-8a38-4c51-a020-aacc8ab937f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628e1f81",
   "metadata": {},
   "source": [
    "# Step 2: loading our dataset.\n",
    "\n",
    "We're using the **with** statement with the **open** function to open a file, use it and then close it back again after its contents have been read.\n",
    "\n",
    "the **'r'** here signifies that we're only reading the contents of the file, and not making changes.\n",
    "\n",
    "**encoding='utf-8'** specifies the character encoding of the file, which in our case is UTF-8, a very common, widely supported encoding format.\n",
    "\n",
    "**as data** assigns our opened txt file to a variable called **data** now the file is accessible to us in this indented code block.\n",
    "\n",
    "**text = data.read()** reads the entire file as a single string.\n",
    "\n",
    "**_Note:_** we're reading the entire file as a single string. This means the computer will load _the entire file at once_ into memory. Our data set here is made of 42,667 characters, for a total memory cost of 42 kilobytes. With 16 gigabytes of available RAM and 8 gigabytes of dedicated VRAM (Video RAM) on the system, this is a non-issue for us, however, commercially used data sets have sizes ranging from a few hundred GB's to a TB of memory, and dedicated memory management methods might be required. One such method is using **for line in data** method, which can be used to read the data line by line and store one by one in an array, commonly used to read CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c55110",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('.\\\\TheFinalProblem.txt' , 'r', encoding='utf-8') as data:\n",
    "    text = data.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15942703",
   "metadata": {},
   "source": [
    "# Step 3: creating tokens\n",
    "\n",
    "This is the first pre-processing step for training our AI model. Here, the **Tokenizer** class converts a text into a matrix of token counts.\n",
    "\n",
    "**tokenizer.fit_on_texts([text]):** updates the vocabulary of the Tokenizer based on the text we just provided.\n",
    "\n",
    "**wordlength = len(tokenizer.word_index) +1** is used to store the number of tokens generated. **word_index** is an index each word of the dataset with a unique number associated with the word. the +1 adds a 1 to the final number, because the counting begins from 0 in case of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e50cc728",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = Tokenizer()\n",
    "tokens.fit_on_texts([text])\n",
    "wordlength = len(tokens.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b56ed",
   "metadata": {},
   "source": [
    "# Step 3.5: checking our tokens\n",
    "\n",
    "we can check the tokens we just created and the indices by using **tokens.word_index** which prints the first 1000 indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf7cc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'i': 2,\n",
       " 'to': 3,\n",
       " 'of': 4,\n",
       " 'and': 5,\n",
       " 'a': 6,\n",
       " 'that': 7,\n",
       " 'in': 8,\n",
       " 'he': 9,\n",
       " 'it': 10,\n",
       " 'my': 11,\n",
       " 'was': 12,\n",
       " 'you': 13,\n",
       " 'his': 14,\n",
       " 'which': 15,\n",
       " 'have': 16,\n",
       " 'had': 17,\n",
       " 'is': 18,\n",
       " 'me': 19,\n",
       " 'as': 20,\n",
       " 'at': 21,\n",
       " 'for': 22,\n",
       " 'but': 23,\n",
       " 'we': 24,\n",
       " 'with': 25,\n",
       " 'an': 26,\n",
       " 'be': 27,\n",
       " 'him': 28,\n",
       " 'from': 29,\n",
       " 'on': 30,\n",
       " 'upon': 31,\n",
       " 'by': 32,\n",
       " 'will': 33,\n",
       " 'said': 34,\n",
       " 'been': 35,\n",
       " 'would': 36,\n",
       " 'there': 37,\n",
       " 'not': 38,\n",
       " 'could': 39,\n",
       " 'our': 40,\n",
       " \"'\": 41,\n",
       " 'this': 42,\n",
       " 'holmes': 43,\n",
       " 'no': 44,\n",
       " 'so': 45,\n",
       " 'all': 46,\n",
       " 'what': 47,\n",
       " 'one': 48,\n",
       " 'watson': 49,\n",
       " 'man': 50,\n",
       " 'up': 51,\n",
       " 'has': 52,\n",
       " 'if': 53,\n",
       " 'over': 54,\n",
       " 'moriarty': 55,\n",
       " 'your': 56,\n",
       " 'then': 57,\n",
       " 'were': 58,\n",
       " 'should': 59,\n",
       " 'when': 60,\n",
       " 'only': 61,\n",
       " 'now': 62,\n",
       " 'into': 63,\n",
       " 'are': 64,\n",
       " 'do': 65,\n",
       " 'out': 66,\n",
       " 'see': 67,\n",
       " 'away': 68,\n",
       " 'may': 69,\n",
       " 'down': 70,\n",
       " 'who': 71,\n",
       " 'last': 72,\n",
       " 'some': 73,\n",
       " 'time': 74,\n",
       " 'two': 75,\n",
       " 'professor': 76,\n",
       " 'way': 77,\n",
       " 'well': 78,\n",
       " 'never': 79,\n",
       " 'us': 80,\n",
       " 'am': 81,\n",
       " 'must': 82,\n",
       " 'little': 83,\n",
       " 'they': 84,\n",
       " 'come': 85,\n",
       " 'tell': 86,\n",
       " 'very': 87,\n",
       " 'came': 88,\n",
       " 'more': 89,\n",
       " 'any': 90,\n",
       " 'think': 91,\n",
       " 'again': 92,\n",
       " 'take': 93,\n",
       " 'before': 94,\n",
       " 'its': 95,\n",
       " 'than': 96,\n",
       " 'round': 97,\n",
       " 'them': 98,\n",
       " 'about': 99,\n",
       " 'left': 100,\n",
       " 'most': 101,\n",
       " 'say': 102,\n",
       " 'shall': 103,\n",
       " 'friend': 104,\n",
       " 'hand': 105,\n",
       " 'far': 106,\n",
       " 'three': 107,\n",
       " 'after': 108,\n",
       " 'myself': 109,\n",
       " 'saw': 110,\n",
       " 'dear': 111,\n",
       " 'where': 112,\n",
       " 'their': 113,\n",
       " 'such': 114,\n",
       " 'through': 115,\n",
       " 'or': 116,\n",
       " 'end': 117,\n",
       " 'path': 118,\n",
       " 'gone': 119,\n",
       " 'few': 120,\n",
       " 'first': 121,\n",
       " 'done': 122,\n",
       " 'london': 123,\n",
       " 'passed': 124,\n",
       " 'made': 125,\n",
       " 'instant': 126,\n",
       " 'can': 127,\n",
       " 'turned': 128,\n",
       " 'street': 129,\n",
       " 'night': 130,\n",
       " 'mr': 131,\n",
       " 'however': 132,\n",
       " 'know': 133,\n",
       " 'english': 134,\n",
       " 'place': 135,\n",
       " 'between': 136,\n",
       " 'still': 137,\n",
       " 'find': 138,\n",
       " 'look': 139,\n",
       " 'face': 140,\n",
       " 'career': 141,\n",
       " 'much': 142,\n",
       " 'great': 143,\n",
       " 'every': 144,\n",
       " 'found': 145,\n",
       " 'get': 146,\n",
       " 'police': 147,\n",
       " 'off': 148,\n",
       " 'morning': 149,\n",
       " \"'you\": 150,\n",
       " 'already': 151,\n",
       " 'make': 152,\n",
       " 'clear': 153,\n",
       " 'against': 154,\n",
       " 'fall': 155,\n",
       " 'black': 156,\n",
       " 'carriage': 157,\n",
       " 'did': 158,\n",
       " 'these': 159,\n",
       " 'ever': 160,\n",
       " 'good': 161,\n",
       " 'rather': 162,\n",
       " 'something': 163,\n",
       " 'close': 164,\n",
       " 'back': 165,\n",
       " 'eyes': 166,\n",
       " 'dangerous': 167,\n",
       " 'half': 168,\n",
       " 'yet': 169,\n",
       " 'without': 170,\n",
       " 'luggage': 171,\n",
       " 'spray': 172,\n",
       " 'rock': 173,\n",
       " 'hotel': 174,\n",
       " 'fashion': 175,\n",
       " 'together': 176,\n",
       " 'matter': 177,\n",
       " 'nothing': 178,\n",
       " 'lay': 179,\n",
       " 'cases': 180,\n",
       " 'too': 181,\n",
       " 'late': 182,\n",
       " 'same': 183,\n",
       " 'danger': 184,\n",
       " 'might': 185,\n",
       " 'quite': 186,\n",
       " 'reached': 187,\n",
       " 'criminal': 188,\n",
       " 'forever': 189,\n",
       " 'presence': 190,\n",
       " 'himself': 191,\n",
       " 'days': 192,\n",
       " 'monday': 193,\n",
       " 'taken': 194,\n",
       " 'side': 195,\n",
       " 'necessary': 196,\n",
       " 'assured': 197,\n",
       " 'return': 198,\n",
       " 'day': 199,\n",
       " 'kept': 200,\n",
       " 'letter': 201,\n",
       " 'waiting': 202,\n",
       " 'station': 203,\n",
       " 'vain': 204,\n",
       " 'train': 205,\n",
       " 'along': 206,\n",
       " 'meiringen': 207,\n",
       " 'alpine': 208,\n",
       " 'swiss': 209,\n",
       " 'text': 210,\n",
       " 'words': 211,\n",
       " 'sherlock': 212,\n",
       " 'feel': 213,\n",
       " 'chance': 214,\n",
       " 'years': 215,\n",
       " 'public': 216,\n",
       " 'second': 217,\n",
       " 'took': 218,\n",
       " 'start': 219,\n",
       " 'companion': 220,\n",
       " 'until': 221,\n",
       " 'during': 222,\n",
       " 'therefore': 223,\n",
       " 'walk': 224,\n",
       " 'room': 225,\n",
       " 'evening': 226,\n",
       " 'even': 227,\n",
       " 'yes': 228,\n",
       " 'wall': 229,\n",
       " 'asked': 230,\n",
       " 'air': 231,\n",
       " 'drew': 232,\n",
       " 'leave': 233,\n",
       " 'house': 234,\n",
       " 'she': 235,\n",
       " \"holmes's\": 236,\n",
       " 'society': 237,\n",
       " 'own': 238,\n",
       " 'chair': 239,\n",
       " 'thought': 240,\n",
       " 'mathematical': 241,\n",
       " 'behind': 242,\n",
       " 'deep': 243,\n",
       " 'those': 244,\n",
       " 'plans': 245,\n",
       " 'paper': 246,\n",
       " 'case': 247,\n",
       " 'whole': 248,\n",
       " 'impossible': 249,\n",
       " 'gang': 250,\n",
       " 'move': 251,\n",
       " 'slip': 252,\n",
       " 'stood': 253,\n",
       " 'pocket': 254,\n",
       " \"'i\": 255,\n",
       " 'minutes': 256,\n",
       " 'stand': 257,\n",
       " 'under': 258,\n",
       " 'rose': 259,\n",
       " 'game': 260,\n",
       " 'leads': 261,\n",
       " 'rooms': 262,\n",
       " 'victoria': 263,\n",
       " 'hansom': 264,\n",
       " 'marked': 265,\n",
       " 'seen': 266,\n",
       " 'hour': 267,\n",
       " 'hardly': 268,\n",
       " 'water': 269,\n",
       " 'stock': 270,\n",
       " 'write': 271,\n",
       " 'record': 272,\n",
       " 'endeavored': 273,\n",
       " 'give': 274,\n",
       " 'account': 275,\n",
       " 'strange': 276,\n",
       " 'study': 277,\n",
       " 'forced': 278,\n",
       " 'recent': 279,\n",
       " 'memory': 280,\n",
       " 'brother': 281,\n",
       " 'absolute': 282,\n",
       " 'papers': 283,\n",
       " 'really': 284,\n",
       " 'practice': 285,\n",
       " 'spring': 286,\n",
       " 'looking': 287,\n",
       " 'remarked': 288,\n",
       " 'answer': 289,\n",
       " 'shutters': 290,\n",
       " 'table': 291,\n",
       " 'mean': 292,\n",
       " 'enough': 293,\n",
       " 'understand': 294,\n",
       " 'smoke': 295,\n",
       " 'contrary': 296,\n",
       " 'break': 297,\n",
       " 'indeed': 298,\n",
       " 'nature': 299,\n",
       " 'question': 300,\n",
       " 'situation': 301,\n",
       " 'heard': 302,\n",
       " 'cried': 303,\n",
       " 'crime': 304,\n",
       " 'beat': 305,\n",
       " 'free': 306,\n",
       " 'ourselves': 307,\n",
       " 'position': 308,\n",
       " 'quiet': 309,\n",
       " 'appearance': 310,\n",
       " 'being': 311,\n",
       " 'powers': 312,\n",
       " 'compelled': 313,\n",
       " 'set': 314,\n",
       " 'known': 315,\n",
       " 'world': 316,\n",
       " 'power': 317,\n",
       " 'law': 318,\n",
       " 'deduced': 319,\n",
       " 'thousand': 320,\n",
       " 'like': 321,\n",
       " 'agents': 322,\n",
       " 'caught': 323,\n",
       " 'organization': 324,\n",
       " 'confess': 325,\n",
       " 'intellectual': 326,\n",
       " 'horror': 327,\n",
       " 'lost': 328,\n",
       " 'trip': 329,\n",
       " 'ready': 330,\n",
       " 'hands': 331,\n",
       " 'written': 332,\n",
       " 'cut': 333,\n",
       " 'business': 334,\n",
       " 'standing': 335,\n",
       " 'tall': 336,\n",
       " 'less': 337,\n",
       " \"'it\": 338,\n",
       " 'evidently': 339,\n",
       " 'crossed': 340,\n",
       " 'mind': 341,\n",
       " 'absolutely': 342,\n",
       " 'foot': 343,\n",
       " 'bring': 344,\n",
       " 'other': 345,\n",
       " 'course': 346,\n",
       " 'convinced': 347,\n",
       " 'van': 348,\n",
       " 'walked': 349,\n",
       " 'roof': 350,\n",
       " 'better': 351,\n",
       " 'spent': 352,\n",
       " 'front': 353,\n",
       " 'here': 354,\n",
       " 'though': 355,\n",
       " 'arcade': 356,\n",
       " 'reach': 357,\n",
       " 'small': 358,\n",
       " 'brougham': 359,\n",
       " 'fellow': 360,\n",
       " 'hurried': 361,\n",
       " 'precaution': 362,\n",
       " 'direction': 363,\n",
       " 'figure': 364,\n",
       " 'italian': 365,\n",
       " 'having': 366,\n",
       " 'returned': 367,\n",
       " 'fear': 368,\n",
       " 'how': 369,\n",
       " 'rapidly': 370,\n",
       " 'canterbury': 371,\n",
       " 'right': 372,\n",
       " 'newhaven': 373,\n",
       " 'mark': 374,\n",
       " 'overtaken': 375,\n",
       " 'doubt': 376,\n",
       " 'certainly': 377,\n",
       " 'old': 378,\n",
       " 'green': 379,\n",
       " 'steiler': 380,\n",
       " 'rosenlaui': 381,\n",
       " 'hill': 382,\n",
       " 'gleam': 383,\n",
       " 'leaving': 384,\n",
       " 'her': 385,\n",
       " 'whom': 386,\n",
       " 'final': 387,\n",
       " 'heavy': 388,\n",
       " 'heart': 389,\n",
       " 'singular': 390,\n",
       " 'deeply': 391,\n",
       " 'brought': 392,\n",
       " 'interference': 393,\n",
       " 'effect': 394,\n",
       " 'intention': 395,\n",
       " 'stopped': 396,\n",
       " 'life': 397,\n",
       " 'letters': 398,\n",
       " 'facts': 399,\n",
       " 'exactly': 400,\n",
       " 'alone': 401,\n",
       " 'purpose': 402,\n",
       " 'served': 403,\n",
       " 'de': 404,\n",
       " '1891': 405,\n",
       " 'finally': 406,\n",
       " 'extremely': 407,\n",
       " 'while': 408,\n",
       " 'extent': 409,\n",
       " 'modified': 410,\n",
       " 'desired': 411,\n",
       " 'year': 412,\n",
       " 'retain': 413,\n",
       " 'winter': 414,\n",
       " 'engaged': 415,\n",
       " 'french': 416,\n",
       " 'importance': 417,\n",
       " 'gathered': 418,\n",
       " 'stay': 419,\n",
       " 'long': 420,\n",
       " 'surprise': 421,\n",
       " 'april': 422,\n",
       " 'pressed': 423,\n",
       " 'light': 424,\n",
       " 'lamp': 425,\n",
       " 'reading': 426,\n",
       " 'afraid': 427,\n",
       " 'means': 428,\n",
       " 'courage': 429,\n",
       " 'refuse': 430,\n",
       " 'recognize': 431,\n",
       " 'trouble': 432,\n",
       " 'cigarette': 433,\n",
       " 'further': 434,\n",
       " 'beg': 435,\n",
       " 'allow': 436,\n",
       " 'garden': 437,\n",
       " 'does': 438,\n",
       " 'knuckles': 439,\n",
       " 'mrs': 440,\n",
       " 'week': 441,\n",
       " 'continent': 442,\n",
       " 'oh': 443,\n",
       " 'pale': 444,\n",
       " 'nerves': 445,\n",
       " 'finger': 446,\n",
       " 'explained': 447,\n",
       " 'probably': 448,\n",
       " 'genius': 449,\n",
       " 'wonder': 450,\n",
       " 'thing': 451,\n",
       " 'pinnacle': 452,\n",
       " 'turn': 453,\n",
       " 'line': 454,\n",
       " 'live': 455,\n",
       " 'congenial': 456,\n",
       " 'rest': 457,\n",
       " 'walking': 458,\n",
       " 'extraordinary': 459,\n",
       " 'excellent': 460,\n",
       " 'smaller': 461,\n",
       " 'brilliant': 462,\n",
       " 'kind': 463,\n",
       " 'dark': 464,\n",
       " 'coach': 465,\n",
       " 'aware': 466,\n",
       " 'knows': 467,\n",
       " 'past': 468,\n",
       " 'conscious': 469,\n",
       " 'wrong': 470,\n",
       " 'crimes': 471,\n",
       " 'web': 472,\n",
       " 'quiver': 473,\n",
       " 'each': 474,\n",
       " 'numerous': 475,\n",
       " 'organized': 476,\n",
       " 'word': 477,\n",
       " 'agent': 478,\n",
       " 'energy': 479,\n",
       " 'breaking': 480,\n",
       " 'evidence': 481,\n",
       " 'convict': 482,\n",
       " 'months': 483,\n",
       " 'afford': 484,\n",
       " 'point': 485,\n",
       " 'net': 486,\n",
       " 'next': 487,\n",
       " 'matters': 488,\n",
       " 'step': 489,\n",
       " 'draw': 490,\n",
       " 'often': 491,\n",
       " 'contest': 492,\n",
       " 'work': 493,\n",
       " 'just': 494,\n",
       " 'complete': 495,\n",
       " 'door': 496,\n",
       " 'fairly': 497,\n",
       " 'thin': 498,\n",
       " 'white': 499,\n",
       " 'curve': 500,\n",
       " 'head': 501,\n",
       " 'shoulders': 502,\n",
       " 'rounded': 503,\n",
       " 'slowly': 504,\n",
       " 'peered': 505,\n",
       " 'development': 506,\n",
       " 'expected': 507,\n",
       " 'fact': 508,\n",
       " 'extreme': 509,\n",
       " 'personal': 510,\n",
       " 'laid': 511,\n",
       " 'smiled': 512,\n",
       " 'glad': 513,\n",
       " \"'on\": 514,\n",
       " 'evident': 515,\n",
       " 'pray': 516,\n",
       " 'yours': 517,\n",
       " 'merely': 518,\n",
       " 'book': 519,\n",
       " 'placed': 520,\n",
       " 'liberty': 521,\n",
       " 'drop': 522,\n",
       " 'intelligence': 523,\n",
       " 'affair': 524,\n",
       " 'treat': 525,\n",
       " \"'that\": 526,\n",
       " 'destruction': 527,\n",
       " 'full': 528,\n",
       " 'realize': 529,\n",
       " 'pleasure': 530,\n",
       " 'conversation': 531,\n",
       " 'awaits': 532,\n",
       " 'looked': 533,\n",
       " 'hope': 534,\n",
       " 'dock': 535,\n",
       " 'pay': 536,\n",
       " 'cheerfully': 537,\n",
       " 'went': 538,\n",
       " 'peering': 539,\n",
       " 'interview': 540,\n",
       " 'soft': 541,\n",
       " 'precise': 542,\n",
       " 'leaves': 543,\n",
       " 'conviction': 544,\n",
       " 'precautions': 545,\n",
       " 'reason': 546,\n",
       " 'blow': 547,\n",
       " 'best': 548,\n",
       " 'feet': 549,\n",
       " 'crossing': 550,\n",
       " 'horse': 551,\n",
       " 'furiously': 552,\n",
       " 'driven': 553,\n",
       " 'dashed': 554,\n",
       " 'believe': 555,\n",
       " 'cab': 556,\n",
       " 'bludgeon': 557,\n",
       " 'confidence': 558,\n",
       " 'possible': 559,\n",
       " 'connection': 560,\n",
       " 'problems': 561,\n",
       " 'act': 562,\n",
       " \"friend's\": 563,\n",
       " 'sat': 564,\n",
       " 'arrest': 565,\n",
       " 'goes': 566,\n",
       " 'cannot': 567,\n",
       " 'remain': 568,\n",
       " 'morrow': 569,\n",
       " 'handed': 570,\n",
       " 'criminals': 571,\n",
       " 'europe': 572,\n",
       " 'messenger': 573,\n",
       " 'itself': 574,\n",
       " 'drive': 575,\n",
       " 'lowther': 576,\n",
       " 'request': 577,\n",
       " 'stops': 578,\n",
       " 'quarter': 579,\n",
       " 'cloak': 580,\n",
       " 'express': 581,\n",
       " 'go': 582,\n",
       " 'immediately': 583,\n",
       " 'injunctions': 584,\n",
       " 'top': 585,\n",
       " 'due': 586,\n",
       " 'among': 587,\n",
       " 'sign': 588,\n",
       " 'porter': 589,\n",
       " 'broken': 590,\n",
       " 'paris': 591,\n",
       " 'given': 592,\n",
       " 'limited': 593,\n",
       " 'voice': 594,\n",
       " 'lip': 595,\n",
       " 'fire': 596,\n",
       " 'heavens': 597,\n",
       " 'hot': 598,\n",
       " 'spoke': 599,\n",
       " 'glancing': 600,\n",
       " 'later': 601,\n",
       " 'formed': 602,\n",
       " 'baker': 603,\n",
       " 'completely': 604,\n",
       " 'arrested': 605,\n",
       " 'mycroft': 606,\n",
       " 'boat': 607,\n",
       " 'why': 608,\n",
       " 'least': 609,\n",
       " 'country': 610,\n",
       " 'journey': 611,\n",
       " 'wait': 612,\n",
       " 'minute': 613,\n",
       " 'open': 614,\n",
       " 'third': 615,\n",
       " 'put': 616,\n",
       " 'england': 617,\n",
       " 'appeal': 618,\n",
       " 'gemmi': 619,\n",
       " 'pass': 620,\n",
       " 'snow': 621,\n",
       " 'below': 622,\n",
       " 'once': 623,\n",
       " 'remember': 624,\n",
       " 'ridge': 625,\n",
       " 'guide': 626,\n",
       " 'conclusion': 627,\n",
       " 'used': 628,\n",
       " 'remains': 629,\n",
       " 'village': 630,\n",
       " 'landlord': 631,\n",
       " 'reichenbach': 632,\n",
       " 'making': 633,\n",
       " 'abyss': 634,\n",
       " 'shaft': 635,\n",
       " 'chasm': 636,\n",
       " 'glistening': 637,\n",
       " 'near': 638,\n",
       " 'human': 639,\n",
       " 'running': 640,\n",
       " 'addressed': 641,\n",
       " 'within': 642,\n",
       " 'lady': 643,\n",
       " 'friends': 644,\n",
       " 'since': 645,\n",
       " 'young': 646,\n",
       " 'arms': 647,\n",
       " 'destined': 648,\n",
       " 'clearly': 649,\n",
       " 'errand': 650,\n",
       " 'sick': 651,\n",
       " 'none': 652,\n",
       " 'leaning': 653,\n",
       " 'shouted': 654,\n",
       " 'around': 655,\n",
       " 'sheer': 656,\n",
       " 'men': 657,\n",
       " 'happened': 658,\n",
       " 'methods': 659,\n",
       " 'soil': 660,\n",
       " 'lines': 661,\n",
       " 'torn': 662,\n",
       " 'lie': 663,\n",
       " 'warranties': 664,\n",
       " 'problem': 665,\n",
       " 'arthur': 666,\n",
       " 'conan': 667,\n",
       " 'doyle': 668,\n",
       " 'pen': 669,\n",
       " 'gifts': 670,\n",
       " 'distinguished': 671,\n",
       " 'incoherent': 672,\n",
       " 'entirely': 673,\n",
       " 'inadequate': 674,\n",
       " 'experiences': 675,\n",
       " 'company': 676,\n",
       " 'period': 677,\n",
       " 'scarlet': 678,\n",
       " 'naval': 679,\n",
       " 'treaty': 680,\n",
       " 'unquestionable': 681,\n",
       " 'preventing': 682,\n",
       " 'serious': 683,\n",
       " 'international': 684,\n",
       " 'complication': 685,\n",
       " 'event': 686,\n",
       " 'created': 687,\n",
       " 'void': 688,\n",
       " 'lapse': 689,\n",
       " 'fill': 690,\n",
       " 'colonel': 691,\n",
       " 'james': 692,\n",
       " 'defends': 693,\n",
       " 'choice': 694,\n",
       " 'occurred': 695,\n",
       " 'truth': 696,\n",
       " 'satisfied': 697,\n",
       " 'suppression': 698,\n",
       " 'accounts': 699,\n",
       " 'press': 700,\n",
       " 'journal': 701,\n",
       " 'genã¨ve': 702,\n",
       " '6th': 703,\n",
       " \"reuter's\": 704,\n",
       " 'despatch': 705,\n",
       " '7th': 706,\n",
       " 'alluded': 707,\n",
       " 'condensed': 708,\n",
       " 'show': 709,\n",
       " 'perversion': 710,\n",
       " 'lies': 711,\n",
       " 'remembered': 712,\n",
       " 'marriage': 713,\n",
       " 'subsequent': 714,\n",
       " 'private': 715,\n",
       " 'intimate': 716,\n",
       " 'relations': 717,\n",
       " 'existed': 718,\n",
       " 'became': 719,\n",
       " 'investigation': 720,\n",
       " 'occasions': 721,\n",
       " 'grew': 722,\n",
       " 'seldom': 723,\n",
       " '1890': 724,\n",
       " 'early': 725,\n",
       " 'government': 726,\n",
       " 'supreme': 727,\n",
       " 'received': 728,\n",
       " 'notes': 729,\n",
       " 'dated': 730,\n",
       " 'narbonne': 731,\n",
       " 'nimes': 732,\n",
       " 'france': 733,\n",
       " 'likely': 734,\n",
       " 'consulting': 735,\n",
       " '24th': 736,\n",
       " 'struck': 737,\n",
       " 'paler': 738,\n",
       " 'thinner': 739,\n",
       " 'usual': 740,\n",
       " 'using': 741,\n",
       " 'freely': 742,\n",
       " 'objection': 743,\n",
       " 'closing': 744,\n",
       " 'edged': 745,\n",
       " 'flinging': 746,\n",
       " 'bolted': 747,\n",
       " 'securely': 748,\n",
       " 'guns': 749,\n",
       " 'nervous': 750,\n",
       " 'stupidity': 751,\n",
       " 'match': 752,\n",
       " 'soothing': 753,\n",
       " 'influence': 754,\n",
       " 'grateful': 755,\n",
       " 'apologize': 756,\n",
       " 'calling': 757,\n",
       " 'unconventional': 758,\n",
       " 'presently': 759,\n",
       " 'scrambling': 760,\n",
       " 'held': 761,\n",
       " 'burst': 762,\n",
       " 'bleeding': 763,\n",
       " 'airy': 764,\n",
       " 'smiling': 765,\n",
       " 'solid': 766,\n",
       " 'visit': 767,\n",
       " 'makes': 768,\n",
       " 'easier': 769,\n",
       " 'propose': 770,\n",
       " 'anywhere': 771,\n",
       " \"it's\": 772,\n",
       " 'aimless': 773,\n",
       " 'holiday': 774,\n",
       " 'worn': 775,\n",
       " 'told': 776,\n",
       " 'highest': 777,\n",
       " 'tension': 778,\n",
       " 'putting': 779,\n",
       " 'tips': 780,\n",
       " 'elbows': 781,\n",
       " 'knees': 782,\n",
       " 'aye': 783,\n",
       " \"there's\": 784,\n",
       " 'pervades': 785,\n",
       " \"that's\": 786,\n",
       " 'puts': 787,\n",
       " 'records': 788,\n",
       " 'seriousness': 789,\n",
       " 'summit': 790,\n",
       " 'prepared': 791,\n",
       " 'placid': 792,\n",
       " 'assistance': 793,\n",
       " 'royal': 794,\n",
       " 'family': 795,\n",
       " 'scandinavia': 796,\n",
       " 'republic': 797,\n",
       " 'continue': 798,\n",
       " 'concentrate': 799,\n",
       " 'attention': 800,\n",
       " 'chemical': 801,\n",
       " 'researches': 802,\n",
       " 'sit': 803,\n",
       " 'streets': 804,\n",
       " 'unchallenged': 805,\n",
       " 'birth': 806,\n",
       " 'education': 807,\n",
       " 'endowed': 808,\n",
       " 'phenomenal': 809,\n",
       " 'faculty': 810,\n",
       " 'age': 811,\n",
       " 'twenty': 812,\n",
       " 'wrote': 813,\n",
       " 'treatise': 814,\n",
       " 'binomial': 815,\n",
       " 'theorem': 816,\n",
       " 'european': 817,\n",
       " 'vogue': 818,\n",
       " 'strength': 819,\n",
       " 'won': 820,\n",
       " 'universities': 821,\n",
       " 'hereditary': 822,\n",
       " 'tendencies': 823,\n",
       " 'diabolical': 824,\n",
       " 'strain': 825,\n",
       " 'ran': 826,\n",
       " 'blood': 827,\n",
       " 'instead': 828,\n",
       " 'increased': 829,\n",
       " 'rendered': 830,\n",
       " 'infinitely': 831,\n",
       " 'mental': 832,\n",
       " 'rumors': 833,\n",
       " 'university': 834,\n",
       " 'town': 835,\n",
       " 'eventually': 836,\n",
       " 'resign': 837,\n",
       " 'army': 838,\n",
       " 'telling': 839,\n",
       " 'discovered': 840,\n",
       " 'higher': 841,\n",
       " 'continually': 842,\n",
       " 'malefactor': 843,\n",
       " 'organizing': 844,\n",
       " 'stands': 845,\n",
       " 'throws': 846,\n",
       " 'shield': 847,\n",
       " 'doer': 848,\n",
       " 'varying': 849,\n",
       " 'sorts': 850,\n",
       " 'forgery': 851,\n",
       " 'robberies': 852,\n",
       " 'murders': 853,\n",
       " 'felt': 854,\n",
       " 'force': 855,\n",
       " 'action': 856,\n",
       " 'many': 857,\n",
       " 'undiscovered': 858,\n",
       " 'personally': 859,\n",
       " 'consulted': 860,\n",
       " 'veil': 861,\n",
       " 'shrouded': 862,\n",
       " 'seized': 863,\n",
       " 'thread': 864,\n",
       " 'followed': 865,\n",
       " 'led': 866,\n",
       " 'cunning': 867,\n",
       " 'windings': 868,\n",
       " 'ex': 869,\n",
       " 'celebrity': 870,\n",
       " 'napoleon': 871,\n",
       " 'organizer': 872,\n",
       " 'evil': 873,\n",
       " 'nearly': 874,\n",
       " 'undetected': 875,\n",
       " 'city': 876,\n",
       " 'philosopher': 877,\n",
       " 'abstract': 878,\n",
       " 'thinker': 879,\n",
       " 'brain': 880,\n",
       " 'order': 881,\n",
       " 'sits': 882,\n",
       " 'motionless': 883,\n",
       " 'spider': 884,\n",
       " 'center': 885,\n",
       " 'radiations': 886,\n",
       " 'splendidly': 887,\n",
       " 'abstracted': 888,\n",
       " 'rifled': 889,\n",
       " 'removed': 890,\n",
       " 'carried': 891,\n",
       " 'money': 892,\n",
       " 'bail': 893,\n",
       " 'defence': 894,\n",
       " 'central': 895,\n",
       " 'uses': 896,\n",
       " 'suspected': 897,\n",
       " 'devoted': 898,\n",
       " 'exposing': 899,\n",
       " 'fenced': 900,\n",
       " 'safeguards': 901,\n",
       " 'cunningly': 902,\n",
       " 'devised': 903,\n",
       " 'seemed': 904,\n",
       " 'court': 905,\n",
       " 'met': 906,\n",
       " 'antagonist': 907,\n",
       " 'equal': 908,\n",
       " 'admiration': 909,\n",
       " 'skill': 910,\n",
       " 'starting': 911,\n",
       " 'woven': 912,\n",
       " 'ripe': 913,\n",
       " 'principal': 914,\n",
       " 'members': 915,\n",
       " 'greatest': 916,\n",
       " 'trial': 917,\n",
       " 'century': 918,\n",
       " 'clearing': 919,\n",
       " 'forty': 920,\n",
       " 'mysteries': 921,\n",
       " 'rope': 922,\n",
       " 'prematurely': 923,\n",
       " 'moment': 924,\n",
       " 'knowledge': 925,\n",
       " 'wily': 926,\n",
       " 'toils': 927,\n",
       " 'strove': 928,\n",
       " 'headed': 929,\n",
       " 'detailed': 930,\n",
       " 'silent': 931,\n",
       " 'bit': 932,\n",
       " 'thrust': 933,\n",
       " 'parry': 934,\n",
       " 'history': 935,\n",
       " 'detection': 936,\n",
       " 'risen': 937,\n",
       " 'height': 938,\n",
       " 'hard': 939,\n",
       " 'opponent': 940,\n",
       " 'undercut': 941,\n",
       " 'steps': 942,\n",
       " 'wanted': 943,\n",
       " 'sitting': 944,\n",
       " 'thinking': 945,\n",
       " 'opened': 946,\n",
       " 'proof': 947,\n",
       " 'thoughts': 948,\n",
       " 'threshold': 949,\n",
       " 'familiar': 950,\n",
       " 'forehead': 951,\n",
       " 'domes': 952,\n",
       " 'sunken': 953,\n",
       " 'clean': 954,\n",
       " 'shaven': 955,\n",
       " 'ascetic': 956,\n",
       " 'retaining': 957,\n",
       " 'features': 958,\n",
       " 'protrudes': 959,\n",
       " 'forward': 960,\n",
       " 'oscillating': 961,\n",
       " 'curiously': 962,\n",
       " 'reptilian': 963,\n",
       " 'curiosity': 964,\n",
       " 'puckered': 965,\n",
       " 'frontal': 966,\n",
       " 'habit': 967,\n",
       " 'loaded': 968,\n",
       " 'firearms': 969,\n",
       " \"one's\": 970,\n",
       " 'dressing': 971,\n",
       " 'gown': 972,\n",
       " 'entrance': 973,\n",
       " 'instantly': 974,\n",
       " 'recognized': 975,\n",
       " 'conceivable': 976,\n",
       " 'escape': 977,\n",
       " 'silencing': 978,\n",
       " 'tongue': 979,\n",
       " 'slipped': 980,\n",
       " 'revolver': 981,\n",
       " 'drawer': 982,\n",
       " 'covering': 983,\n",
       " 'cloth': 984,\n",
       " 'remark': 985,\n",
       " 'weapon': 986,\n",
       " 'cocked': 987,\n",
       " 'blinked': 988,\n",
       " \"don't\": 989,\n",
       " 'answered': 990,\n",
       " 'spare': 991,\n",
       " 'five': 992,\n",
       " 'anything': 993,\n",
       " \"'all\": 994,\n",
       " \"'then\": 995,\n",
       " 'possibly': 996,\n",
       " 'replied': 997,\n",
       " 'fast': 998,\n",
       " \"'absolutely\": 999,\n",
       " 'clapped': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc2e0ae",
   "metadata": {},
   "source": [
    "# Step 4: creating our N-grams\n",
    "\n",
    "An N-gram is defined as a sequence of adjacent words in a particular order. N-grams are used in a variety of contexts like Protein sequencing, DNA sequencing and language models.\n",
    "\n",
    "![](./Images/2024-03-03-00-14-17.png)\n",
    "_source: https://botpenguin.com/glossary/n-gram_\n",
    "\n",
    "here, **inputsequence** is our new empty list, where we'll store our n-grams.\n",
    "\n",
    "**for line in text.split('\\n')** this loop goes through every line, split at newline character **'\\n'**.\n",
    "\n",
    "**tokenlist = tokenizer.texts_to_sequences([line])[0]** here, each line is created into a sequence of tokens. Since we are processing line by line, we begin at 0.\n",
    "\n",
    "**for i in range(1, len(token_list)):** this inner loop iterates from the second index (1 in this case) to the last token.\n",
    "\n",
    "**ngramsequence = token_list[:i+1]** here, an n-gram sequece is created including tokens from the beginning up to the current index.\n",
    "\n",
    "**input_sequences.append(ngramsequence)** here, the n-gram sequence is added to the **inputsequence** list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c8adca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputsequence = []\n",
    "for line in text.split('\\n'):\n",
    "    tokenlist = tokens.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(tokenlist)):\n",
    "        ngramsequence = tokenlist[:i+1]\n",
    "        inputsequence.append(ngramsequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8651fa",
   "metadata": {},
   "source": [
    "# Step 5: Setting the maximum sequence length\n",
    "\n",
    "\n",
    "**maxsequencelen = max([len(seq) for seq in input_sequences])** calculates the maximum sequence length from all the sequences in **inputsequence**\n",
    "\n",
    "**input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))** equalizes all the sequences to have the same length by adding padding at the beginning (**pre**) and arranging them into a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7fead25",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxsequencelen = max([len(seq) for seq in inputsequence])\n",
    "inputsequence = np.array(pad_sequences(inputsequence, maxlen=maxsequencelen, padding='pre'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64b4b79",
   "metadata": {},
   "source": [
    "# Step 6: Preparing input and target data\n",
    "\n",
    "now we're assigning parts of **inputsequence** into two seperate variables, **X**(input) and **y** (target)\n",
    "\n",
    "**X** contains all the columns except the last column of **inputsequence**\n",
    "\n",
    "**y** contains only the last column of **inputsequence**\n",
    "\n",
    "therefore, **X** contains the input sequence of tokens and **y** contains the target token, or the token to be \"predicted\" while training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "805c109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = inputsequence[:, :-1]\n",
    "y = inputsequence[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83569743",
   "metadata": {},
   "source": [
    "# Step 7: One-hot encoding\n",
    "\n",
    "One-hot encoding is a method to represent categorical data in the form of binary vectors, or vectors in a matrix where the only values are 0 and 1, and are used to see if a certain index is present or not. This way we can represent individual words in a vector matrix and keep them distinct from each other.\n",
    "\n",
    "for example:\n",
    "\n",
    "Lets take a vocabulary of 5 words: [\"apple\", \"banana\", \"orange\", \"grape\", \"kiwi\"]. Each word in this vocabulary will be represented as a unique index:\n",
    "\n",
    "    \"apple\" is assigned index 0,\n",
    "    \"banana\" is assigned index 1,\n",
    "    \"orange\" is assigned index 2,\n",
    "    \"grape\" is assigned index 3,\n",
    "    \"kiwi\" is assigned index 4.\n",
    "\n",
    "Now, if we have a sentence like \"banana is a fruit,\" and we want to represent each word in this sentence using one-hot encoding, the matrix would end up with the following binary vectors:\n",
    "\n",
    "    \"banana\": [0, 1, 0, 0, 0] (because it's at index 1)\n",
    "    \"is\": [0, 0, 0, 0, 0] (in this case, it's not in the vocabulary of fruits)\n",
    "    \"a\": [0, 0, 0, 0, 0]\n",
    "    \"fruit\": [0, 0, 0, 0, 0]\n",
    "\n",
    "Each vector is the length of the vocabulary (which is five words in this case), and it contains all zeros except for a 1 at the index corresponding to the word.\n",
    "\n",
    "in our code, **tf.keras.utils.to_categorical(y, num_classes=wordlength)** converts the variable **y** into a binary matrix.\n",
    "\n",
    "in order to set the number of classes, we use **num_classes=wordlength** which is the total length of the word index we calculated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e4eedba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=wordlength))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a427e7",
   "metadata": {},
   "source": [
    "# Step 8: building our model\n",
    "\n",
    "So far, we have:\n",
    "\n",
    "- Read our input data.\n",
    "- Converted each word into a token and given it a distinct index number.\n",
    "- Created N-grams, or sequences of words as a sequence of tokens' index number.\n",
    "- Determined a maximum sequence length, and padded all sequences, where necessary, to have all sequences of same length.\n",
    "- Split our sequence data into an input sequence (which contains all but the last word of the sequence) and the target sequence (which contains the last word of the sequence) so the model can be shown to guess words\n",
    "- one-hot encoded the target data, so the model can tell which index is appearing where.\n",
    "\n",
    "Now that we have all the pieces we need to train the model, we build the model.\n",
    "\n",
    "**model = Sequential()** initializes a sequential model, where each layer has one input and one output tensor.\n",
    "\n",
    "**model.add(Embedding(wordlength, 100, input_length=maxsequencelen-1))** adds an embedding layer to the model where the integer indices are converted to dense vectors (vectors where most values are non-zero) of a fixed size, which is why we converted all sequences to a fixed size and one hot encoded our target variable. Here the vector length is 100 dimensions and is a vector where the values are adjusted to make sure that words with similar meanings have similar vector arrangements so that the model can start to understand their usage.\n",
    "\n",
    "**model.add(LSTM(150))** adds the LSTM layer, which has 150 neurons.\n",
    "\n",
    "**model.add(Dense(wordlength, activation='softmax'))** adds a dense output layer to the model, this layer will have a vocabulary size determined by **wordlength**. The activation function here is **softmax** used commonly for multi-classification problems. This function normalizes the output values into a normal probability distribution over the entire vocabulary, to get a better idea of which word will be next.\n",
    "\n",
    "The structure of our model is: \n",
    "\n",
    "![](./Images/2024-03-03-02-09-50.png)\n",
    "\n",
    "To get a summary of the model, we use **print(model.summary())**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce2f67e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 16, 100)           177100    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 150)               150600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1771)              267421    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 595,121\n",
      "Trainable params: 595,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(wordlength, 100, input_length=maxsequencelen-1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(wordlength, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c11b5a",
   "metadata": {},
   "source": [
    "Here we can see, there are no parameters that the model cannot be trained on, which means the data we prepared is problem-free and will cause no problems with training or use of the model.\n",
    "\n",
    "We are now ready to compile the model, and fit it to the data we just prepared.\n",
    "\n",
    "# Step 9: compiling and fitting our model\n",
    "\n",
    "We're compiling our model according to the following parameters:\n",
    "\n",
    "- **Loss function: Categorical Crossentropy**, a common function used for multi-class classification problems\n",
    "\n",
    "- **Optimizer: Adam**, a well known and widely used optimizer, known for its efficiency and effectiveness in Deep learning\n",
    "\n",
    "- **Metrics: accuracy**, which will measure what percentage of the predicted words were actually correct. \n",
    "\n",
    "We're fitting our model with the following parameters:\n",
    "\n",
    "- **Epochs = 100** which means the model will have 100 iteration steps.\n",
    "\n",
    "- **Verbose = 1** which will show a progress bar for each epoch as it iterates through them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "882dfe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "209/209 [==============================] - 7s 6ms/step - loss: 6.3584 - accuracy: 0.0555\n",
      "Epoch 2/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 5.8354 - accuracy: 0.0645\n",
      "Epoch 3/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 5.6759 - accuracy: 0.0726\n",
      "Epoch 4/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 5.5312 - accuracy: 0.0781\n",
      "Epoch 5/100\n",
      "209/209 [==============================] - 1s 6ms/step - loss: 5.3850 - accuracy: 0.0905\n",
      "Epoch 6/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 5.2125 - accuracy: 0.1109\n",
      "Epoch 7/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 5.0369 - accuracy: 0.1244\n",
      "Epoch 8/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 4.8593 - accuracy: 0.1342\n",
      "Epoch 9/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 4.6813 - accuracy: 0.1487\n",
      "Epoch 10/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 4.5090 - accuracy: 0.1587\n",
      "Epoch 11/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 4.3429 - accuracy: 0.1712\n",
      "Epoch 12/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 4.1823 - accuracy: 0.1833\n",
      "Epoch 13/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 4.0251 - accuracy: 0.2009\n",
      "Epoch 14/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 3.8666 - accuracy: 0.2138\n",
      "Epoch 15/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 3.7108 - accuracy: 0.2279\n",
      "Epoch 16/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 3.5526 - accuracy: 0.2443\n",
      "Epoch 17/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 3.3967 - accuracy: 0.2664\n",
      "Epoch 18/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 3.2411 - accuracy: 0.2944\n",
      "Epoch 19/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 3.0888 - accuracy: 0.3188\n",
      "Epoch 20/100\n",
      "209/209 [==============================] - 1s 6ms/step - loss: 2.9365 - accuracy: 0.3494\n",
      "Epoch 21/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 2.7907 - accuracy: 0.3830\n",
      "Epoch 22/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 2.6451 - accuracy: 0.4182\n",
      "Epoch 23/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 2.5035 - accuracy: 0.4470\n",
      "Epoch 24/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 2.3676 - accuracy: 0.4828\n",
      "Epoch 25/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 2.2368 - accuracy: 0.5133\n",
      "Epoch 26/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 2.1104 - accuracy: 0.5428\n",
      "Epoch 27/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 1.9923 - accuracy: 0.5740\n",
      "Epoch 28/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 1.8770 - accuracy: 0.6011\n",
      "Epoch 29/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 1.7729 - accuracy: 0.6212\n",
      "Epoch 30/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 1.6681 - accuracy: 0.6480\n",
      "Epoch 31/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 1.5693 - accuracy: 0.6752\n",
      "Epoch 32/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 1.4774 - accuracy: 0.6993\n",
      "Epoch 33/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 1.3925 - accuracy: 0.7159\n",
      "Epoch 34/100\n",
      "209/209 [==============================] - 1s 6ms/step - loss: 1.3099 - accuracy: 0.7323\n",
      "Epoch 35/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 1.2324 - accuracy: 0.7500\n",
      "Epoch 36/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 1.1572 - accuracy: 0.7675\n",
      "Epoch 37/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 1.0880 - accuracy: 0.7877\n",
      "Epoch 38/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 1.0219 - accuracy: 0.8032\n",
      "Epoch 39/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.9607 - accuracy: 0.8159\n",
      "Epoch 40/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.9038 - accuracy: 0.8294\n",
      "Epoch 41/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.8476 - accuracy: 0.8416\n",
      "Epoch 42/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.7947 - accuracy: 0.8520\n",
      "Epoch 43/100\n",
      "209/209 [==============================] - 1s 6ms/step - loss: 0.7450 - accuracy: 0.8660\n",
      "Epoch 44/100\n",
      "209/209 [==============================] - 1s 6ms/step - loss: 0.6994 - accuracy: 0.8759\n",
      "Epoch 45/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.6564 - accuracy: 0.8807\n",
      "Epoch 46/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.6124 - accuracy: 0.8933\n",
      "Epoch 47/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.5762 - accuracy: 0.9017\n",
      "Epoch 48/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.5413 - accuracy: 0.9075\n",
      "Epoch 49/100\n",
      "209/209 [==============================] - 1s 6ms/step - loss: 0.5071 - accuracy: 0.9140\n",
      "Epoch 50/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.4775 - accuracy: 0.9179\n",
      "Epoch 51/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.4506 - accuracy: 0.9252\n",
      "Epoch 52/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.4228 - accuracy: 0.9286\n",
      "Epoch 53/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.3977 - accuracy: 0.9327\n",
      "Epoch 54/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.3738 - accuracy: 0.9351\n",
      "Epoch 55/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.3536 - accuracy: 0.9361\n",
      "Epoch 56/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.3354 - accuracy: 0.9387\n",
      "Epoch 57/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.3171 - accuracy: 0.9412\n",
      "Epoch 58/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.3026 - accuracy: 0.9405\n",
      "Epoch 59/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.2869 - accuracy: 0.9412\n",
      "Epoch 60/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.2734 - accuracy: 0.9445\n",
      "Epoch 61/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.2614 - accuracy: 0.9432\n",
      "Epoch 62/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.2500 - accuracy: 0.9450\n",
      "Epoch 63/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.2373 - accuracy: 0.9465\n",
      "Epoch 64/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.2298 - accuracy: 0.9475\n",
      "Epoch 65/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.2213 - accuracy: 0.9459\n",
      "Epoch 66/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.2131 - accuracy: 0.9462\n",
      "Epoch 67/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.2072 - accuracy: 0.9460\n",
      "Epoch 68/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.2008 - accuracy: 0.9483\n",
      "Epoch 69/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1943 - accuracy: 0.9457\n",
      "Epoch 70/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1880 - accuracy: 0.9481\n",
      "Epoch 71/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1836 - accuracy: 0.9469\n",
      "Epoch 72/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1794 - accuracy: 0.9472\n",
      "Epoch 73/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1778 - accuracy: 0.9456\n",
      "Epoch 74/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1736 - accuracy: 0.9469\n",
      "Epoch 75/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1685 - accuracy: 0.9469\n",
      "Epoch 76/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1637 - accuracy: 0.9483\n",
      "Epoch 77/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1629 - accuracy: 0.9453\n",
      "Epoch 78/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1605 - accuracy: 0.9472\n",
      "Epoch 79/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1576 - accuracy: 0.9469\n",
      "Epoch 80/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1555 - accuracy: 0.9472\n",
      "Epoch 81/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1548 - accuracy: 0.9457\n",
      "Epoch 82/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1513 - accuracy: 0.9462\n",
      "Epoch 83/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1492 - accuracy: 0.9487\n",
      "Epoch 84/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1483 - accuracy: 0.9463\n",
      "Epoch 85/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1508 - accuracy: 0.9468\n",
      "Epoch 86/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1707 - accuracy: 0.9421\n",
      "Epoch 87/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1573 - accuracy: 0.9472\n",
      "Epoch 88/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1501 - accuracy: 0.9450\n",
      "Epoch 89/100\n",
      "209/209 [==============================] - 1s 6ms/step - loss: 0.1421 - accuracy: 0.9462\n",
      "Epoch 90/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1411 - accuracy: 0.9471\n",
      "Epoch 91/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1404 - accuracy: 0.9456\n",
      "Epoch 92/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1387 - accuracy: 0.9468\n",
      "Epoch 93/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1377 - accuracy: 0.9463\n",
      "Epoch 94/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1376 - accuracy: 0.9469\n",
      "Epoch 95/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1365 - accuracy: 0.9472\n",
      "Epoch 96/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1358 - accuracy: 0.9462\n",
      "Epoch 97/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1355 - accuracy: 0.9478\n",
      "Epoch 98/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1343 - accuracy: 0.9463\n",
      "Epoch 99/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1346 - accuracy: 0.9459\n",
      "Epoch 100/100\n",
      "209/209 [==============================] - 1s 5ms/step - loss: 0.1331 - accuracy: 0.9468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c2f9e907c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98507040",
   "metadata": {},
   "source": [
    "# Step 10: Moment of truth\n",
    "\n",
    "Its time to feed the model a seed text (text it will use as an input to predict) the next words.\n",
    "\n",
    "we will also ask it to predict a certain number of words, in this case, 10.\n",
    "\n",
    "This seed text will be sent through a prediction loop, which: \n",
    "\n",
    "- Converts the current seedtext into a sequence of tokens.\n",
    "- Pads the sequence to match the model's input length.\n",
    "- Uses the trained model to predict the next word.\n",
    "- Converts the predicted word index back into the actual word using tokens's word index.\n",
    "- Appends the predicted word to the seedtext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e989d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "What happens we do this then we must place me to the strand end of\n"
     ]
    }
   ],
   "source": [
    "seedtext = \"What happens we do this\"\n",
    "nextwords = 10\n",
    "\n",
    "for _ in range(nextwords):\n",
    "    token_list = tokens.texts_to_sequences([seedtext])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=maxsequencelen-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokens.word_index.items():\n",
    "        if index == predicted[0]:\n",
    "            output_word = word\n",
    "            break\n",
    "    seedtext += \" \" + output_word\n",
    "\n",
    "print(seedtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcc9065",
   "metadata": {},
   "source": [
    "# Result:\n",
    "\n",
    "Our final result is: _What happens we do this then we must place me to the strand end of_ which resembles a semi-coherent sentence, and is cut off abruptly at the end, which is simply because we asked it to stop when it finished its word quota, and not when it had a coherent idea to present.\n",
    "\n",
    "This is a prime example of the fact that *Artificial Intelligence does what we tell it to do, and not what we want it to do*\n",
    "\n",
    "As the amount of data fed to this model grows, so will the coherency of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556a855c",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "- CS 230 - Recurrent Neural Networks Cheatsheet. (n.d.). https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\n",
    "- Wikipedia contributors. (2024b, February 10). Long short-term memory. Wikipedia. https://en.wikipedia.org/wiki/Long_short-term_memory\n",
    "- İlaslan, D. (2023, September 11). Next Word Prediction using LSTM with TensorFlow - Düzgün İlaslan - Medium. Medium. https://medium.com/@ilaslanduzgun/next-word-prediction-using-lstm-with-tensorflow-e2a8f63b613c\n",
    "- Donges, N. (2024, February 28). A complete guide to Recurrent Neural networks (RNNs). Built In. https://builtin.com/data-science/recurrent-neural-networks-and-lstm\n",
    "- Kharwal, A. (2022, January 3). Stock Price Prediction with LSTM. Thecleverprogrammer. https://thecleverprogrammer.com/2022/01/03/stock-price-prediction-with-lstm/\n",
    "- Wikipedia contributors. (2024c, February 12). Recurrent neural network. Wikipedia. https://en.wikipedia.org/wiki/Recurrent_neural_network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
